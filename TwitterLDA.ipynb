{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unidecode\n",
    "import demoji\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "nlp = spacy.load(\"pt\")\n",
    "nlp.Defaults.stop_words |= {\"to\",\"uol\",\"mi\",\"budddhetg\",\"the\", \"ne\", \"vou\", \"ta\", \"via\",\"ex\", \"pq\", \"vc\",\"aa\",\"pra\",\"to\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_irrelevantes = set(stopwords.words('portuguese') + list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/tweeterLeo/dataTweeter.csv\", sep=\";\", encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(tweet):\n",
    "    if \"|\" in tweet:\n",
    "        tweet = tweet.split(\"|\")[1]\n",
    "        \n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([0-9])\", \" \", tweet).split())\n",
    "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    tweet = ' '.join(re.sub(\"[\\_\\|\\.\\,\\\"\\'\\!\\?\\:\\;\\$\\-\\(\\)\\=]\", \" \", tweet).split())\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    le = list(demoji.findall(tweet))\n",
    "    for i in le:\n",
    "        tweet = tweet.replace(i, \"\")\n",
    "    \n",
    "    if tweet.startswith('rt '):\n",
    "        tweet = tweet.replace(\"rt \", \"\")\n",
    "    \n",
    "    lNewTweet = []\n",
    "    for i in tweet.split(\" \"):\n",
    "        if i not in palavras_irrelevantes and i not in STOP_WORDS:\n",
    "            lNewTweet.append(i)\n",
    "    \n",
    "    newTweet = \" \".join(lNewTweet)\n",
    "        \n",
    "    return unidecode.unidecode(newTweet.replace(\" rt \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tweet_text_clean\"] = data.text.apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>topic</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1,31497377565871E+018</td>\n",
       "      <td>DivaDepressao</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @PaivaPaulo8: @DivaDepressao pelo amor de D...</td>\n",
       "      <td>amor deus analisa festa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1,31497371872942E+018</td>\n",
       "      <td>DivaDepressao</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @andrecarboni666: meu vício agora é pegar ô...</td>\n",
       "      <td>vicio onibus ouvindo filhos gravida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1,31497369440248E+018</td>\n",
       "      <td>DivaDepressao</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @bii_aa: Alô dona @NetflixBrasil a senhora ...</td>\n",
       "      <td>alo dona senhora assistiu video viu hahahahhah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1,3149736672863E+018</td>\n",
       "      <td>DivaDepressao</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@SandroArturBel1 @JornalOGlobo Ihhhhhh</td>\n",
       "      <td>ihhhhhh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1,31497360792013E+018</td>\n",
       "      <td>DivaDepressao</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @fatbarbiemarley: @FilhosDaGravida o álcool...</td>\n",
       "      <td>alcool planta edu fala arnica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id           user topic  \\\n",
       "0  1,31497377565871E+018  DivaDepressao   NaN   \n",
       "1  1,31497371872942E+018  DivaDepressao   NaN   \n",
       "2  1,31497369440248E+018  DivaDepressao   NaN   \n",
       "3   1,3149736672863E+018  DivaDepressao   NaN   \n",
       "4  1,31497360792013E+018  DivaDepressao   NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  RT @PaivaPaulo8: @DivaDepressao pelo amor de D...   \n",
       "1  RT @andrecarboni666: meu vício agora é pegar ô...   \n",
       "2  RT @bii_aa: Alô dona @NetflixBrasil a senhora ...   \n",
       "3             @SandroArturBel1 @JornalOGlobo Ihhhhhh   \n",
       "4  RT @fatbarbiemarley: @FilhosDaGravida o álcool...   \n",
       "\n",
       "                                    tweet_text_clean  \n",
       "0                            amor deus analisa festa  \n",
       "1                vicio onibus ouvindo filhos gravida  \n",
       "2  alo dona senhora assistiu video viu hahahahhah...  \n",
       "3                                            ihhhhhh  \n",
       "4                      alcool planta edu fala arnica  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "todas_palavras = ' '.join([texto for texto in data[\"tweet_text_clean\"]])\n",
    "\n",
    "token_espaco = tokenize.WhitespaceTokenizer()\n",
    "token_frase = token_espaco.tokenize(todas_palavras)\n",
    "\n",
    "token_frase_2 = []\n",
    "\n",
    "for i in token_frase:\n",
    "    a = cleanText(i)\n",
    "    if a:\n",
    "        token_frase_2.append(cleanText(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencia = nltk.FreqDist(token_frase_2)\n",
    "freqmc = frequencia.most_common(10000)\n",
    "\n",
    "# freqmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Gemsin bigrams\n",
    "# token_frase_2\n",
    "# lista_texto = [texto[0] for texto in token_frase_2]\n",
    "# lista_texto\n",
    "# # # Build the bigram and trigram models\n",
    "# t = data[\"tweet_text_clean\"].tolist()\n",
    "# t\n",
    "\n",
    "token_ = [doc.split(\" \") for doc in data[\"tweet_text_clean\"].tolist()]\n",
    "\n",
    "# # token_ = [todas_palavras]\n",
    "import gensim\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "bigram = gensim.models.Phrases(token_, min_count=1, threshold=0.1) # higher threshold fewer phrases\n",
    "trigram = gensim.models.Phrases(bigram[token_], threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "l = [trigram_mod[bigram_mod[doc]] for doc in token_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dir(bigram_mod)\n",
    "# # l[6000:]\n",
    "import gensim.corpora as corpora\n",
    "id2word = corpora.Dictionary(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.030*\"covid\" + 0.008*\"elenco\" + 0.007*\"perfeito\" + 0.006*\"nome\" + '\n",
      "  '0.006*\"brasileira\" + 0.006*\"temporada\" + 0.006*\"chegou\" + 0.006*\"play\" + '\n",
      "  '0.005*\"alta\" + 0.005*\"domingo\"'),\n",
      " (1,\n",
      "  '0.024*\"brasil\" + 0.008*\"melhor\" + 0.007*\"calma\" + 0.006*\"chega_dia\" + '\n",
      "  '0.005*\"pandemia\" + 0.005*\"milhoes\" + 0.005*\"debate\" + 0.005*\"amor\" + '\n",
      "  '0.005*\"mostra\" + 0.004*\"livro\"'),\n",
      " (2,\n",
      "  '0.007*\"vida\" + 0.007*\"rio\" + 0.006*\"fala\" + 0.006*\"natal\" + 0.006*\"onu\" + '\n",
      "  '0.005*\"pt\" + 0.005*\"dias\" + 0.005*\"viu\" + 0.005*\"feito\" + 0.005*\"tv\"'),\n",
      " (3,\n",
      "  '0.014*\"governo\" + 0.009*\"historia\" + 0.007*\"policia\" + 0.007*\"acao\" + '\n",
      "  '0.006*\"queimadas\" + 0.006*\"candidatos\" + 0.006*\"disse\" + 0.005*\"pantanal\" + '\n",
      "  '0.005*\"musica\" + 0.005*\"twitter\"'),\n",
      " (4,\n",
      "  '0.015*\"eua\" + 0.009*\"presidente\" + 0.007*\"chega\" + 0.006*\"donald_trump\" + '\n",
      "  '0.005*\"ministro\" + 0.005*\"o\" + 0.004*\"manha\" + 0.004*\"amanha\" + '\n",
      "  '0.004*\"queda\" + 0.004*\"mercado\"'),\n",
      " (5,\n",
      "  '0.009*\"outubro\" + 0.005*\"ai\" + 0.005*\"hospital\" + 0.005*\"homem\" + '\n",
      "  '0.004*\"outro\" + 0.004*\"lula\" + 0.004*\"queria\" + 0.004*\"assista\" + '\n",
      "  '0.003*\"cidade\" + 0.003*\"politico\"'),\n",
      " (6,\n",
      "  '0.016*\"ne\" + 0.015*\"falar\" + 0.008*\"diabo_dia\" + 0.007*\"familia\" + '\n",
      "  '0.006*\"talento\" + 0.006*\"dinheiro\" + 0.005*\"volta_aulas\" + 0.005*\"sc\" + '\n",
      "  '0.005*\"voz\" + 0.005*\"semana\"'),\n",
      " (7,\n",
      "  '0.014*\"trump\" + 0.011*\"coronavirus\" + 0.010*\"stf\" + 0.008*\"mundo\" + '\n",
      "  '0.007*\"guedes\" + 0.006*\"fica\" + 0.006*\"eleicoes\" + '\n",
      "  '0.006*\"assista_sinal_aberto\" + 0.005*\"vivo\" + 0.004*\"leia_blog\"'),\n",
      " (8,\n",
      "  '0.024*\"bolsonaro\" + 0.017*\"sp\" + 0.011*\"anos\" + 0.009*\"ano\" + 0.008*\"filme\" '\n",
      "  '+ 0.007*\"mulher\" + 0.006*\"paulo\" + 0.005*\"quase\" + 0.005*\"caso\" + '\n",
      "  '0.005*\"hoje\"'),\n",
      " (9,\n",
      "  '0.013*\"serie\" + 0.013*\"opiniao\" + 0.011*\"to\" + 0.007*\"dia\" + 0.006*\"rj\" + '\n",
      "  '0.006*\"feira\" + 0.005*\"lava_jato\" + 0.005*\"julie_and_phantoms\" + '\n",
      "  '0.005*\"jair_bolsonaro\" + 0.004*\"perfeita\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.6500950642994371\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=l, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
